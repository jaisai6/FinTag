{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Dataset"
      ],
      "metadata": {
        "id": "eepLNILWDZcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLQnyi3NHBpH",
        "outputId": "b09a729b-7c83-428a-a748-6ccee0e056c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=34cc15cf588f248474b0eab786f9f1c2fe5081f2a03d29ca3456f798d1770689\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.14.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l8Sn3ForuB6u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "\n",
        "# from scipy.spatial import distance\n",
        "# import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "# sen_embeddings = model.encode(ood_data, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "SGesOx-bVlAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NCD"
      ],
      "metadata": {
        "id": "BKY1zhlCbQG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_annotations=[]\n",
        "all_labels=[]"
      ],
      "metadata": {
        "id": "mp6sr3Xibhu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "# sen_embeddings = model.encode(ood_data, convert_to_tensor=True)\n",
        "\n",
        "# Use BERT encoders to \n",
        "\n",
        "# Novel Class Detection\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 2\n",
        "random.seed(0)\n",
        "while(1):\n",
        "    if new_classes > num_clusters:\n",
        "        # Define kmeans model\n",
        "        random.seed(1)\n",
        "        clustering_model = KMeans(n_clusters=num_clusters,random_state=0)\n",
        "        # Fit the embedding with kmeans clustering.\n",
        "        clustering_model.fit(sen_embeddings)\n",
        "        cluster_assignment = clustering_model.labels_\n",
        "        clustered_sentences = [[] for i in range(num_clusters)]\n",
        "        for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "            clustered_sentences[cluster_id].append(ood_data[sentence_id])\n",
        "\n",
        "        annotations=[]\n",
        "        for i in range(len(clustered_sentences)):\n",
        "            annotations.extend(random.sample(clustered_sentences[i],2))\n",
        "        all_annotations.extend(annotations)\n",
        "        labels_for_annotation=[]\n",
        "        for data in annotations:\n",
        "            idx= valid_data.index(data)\n",
        "            labels_for_annotation.append(valid_labels[idx])\n",
        "        all_labels.extend(labels_for_annotation)\n",
        "        new_classes = len(set(labels_for_annotation)-set(train_labels))\n",
        "        print(f'No of new classes discovered:{new_classes}')\n",
        "        num_clusters*=2\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "id": "AKSKh8g_ax5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CQBA"
      ],
      "metadata": {
        "id": "8mUR_iG3bM5e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VL0IxMMV5rOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "cluster_data={}\n",
        "cluster_data_annotation=defaultdict(list)\n",
        "cluster_data_annotation_labels = defaultdict(list)\n",
        "all_new_data=set()\n",
        "for i in range(len(clustered_sentences)):\n",
        "    data_pts = clustered_sentences[i]\n",
        "    intersection = set(data_pts) & set(all_annotations)\n",
        "    if len(intersection)==0:\n",
        "        cluster_data[i].append([])\n",
        "    else:\n",
        "        labels_cluster=[]\n",
        "        for ann in intersection:\n",
        "            idx = all_annotations.index(ann)\n",
        "            if all_labels[idx] not in train_labels:\n",
        "                labels_cluster.append(all_labels[idx])\n",
        "                cluster_data_annotation[i].append(ann)\n",
        "                cluster_data_annotation_labels[i].append(all_labels[idx])\n",
        "        cluster_data[i]=set(labels_cluster)\n",
        "        for label in labels_cluster:\n",
        "            all_new_data.add(label)\n"
      ],
      "metadata": {
        "id": "gYlPieiv-XIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cluster=defaultdict(list)\n",
        "for cluster in cluster_data:\n",
        "    values = cluster_data[cluster]\n",
        "    for v in values:\n",
        "        transform_cluster[v].append(cluster)\n"
      ],
      "metadata": {
        "id": "WCCRers-brNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "good_cluster=[]\n",
        "bad_cluster=[]\n",
        "for key in set(cluster_data_annotation_labels):\n",
        "    if len(set(cluster_data[key]))==1:\n",
        "        good_cluster.append(key)\n",
        "    else:\n",
        "        bad_cluster.append(key)"
      ],
      "metadata": {
        "id": "LXqzBdapbsgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_annotation_good_cluster = []\n",
        "label_annotation_good_cluster=[]\n",
        "annotation_embs=[]\n",
        "data_annotation_bad_cluster=[]\n",
        "label_annotation_bad_cluster=[]\n",
        "bad_annotation_embs=[]\n",
        "for cluster in good_cluster:\n",
        "    for ann in cluster_data_annotation[cluster]:\n",
        "        data_annotation_good_cluster.append(ann)\n",
        "        annotation_embs.append(sen_embeddings[ood_data.index(ann)])\n",
        "        idx = valid_data.index(ann)\n",
        "        label_annotation_good_cluster.append(valid_labels[idx])\n",
        "        #print(cluster)\n",
        "for cluster in bad_cluster:\n",
        "    for ann in cluster_data_annotation[cluster]:\n",
        "        data_annotation_bad_cluster.append(ann)\n",
        "        bad_annotation_embs.append(sen_embeddings[ood_data.index(ann)])\n",
        "        idx = valid_data.index(ann)\n",
        "        label_annotation_bad_cluster.append(valid_labels[idx])"
      ],
      "metadata": {
        "id": "yU6zuJ6rbuaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_good_clusters=[]\n",
        "new_silver_data = []\n",
        "new_silver_class = []\n",
        "count=0\n",
        "random.seed(1)\n",
        "cluster_wise_pts_remaining=defaultdict(list)\n",
        "for cluster in good_cluster:\n",
        "    already_annotated = cluster_data_annotation[cluster]\n",
        "    if len(already_annotated)<2:\n",
        "        data = random.sample(clustered_sentences[cluster], 2- len(already_annotated))\n",
        "        count+=len(data)\n",
        "        data.extend(already_annotated)\n",
        "    else:\n",
        "        data = already_annotated\n",
        "        #print(data)\n",
        "    lbl_set=set()\n",
        "    for d in data:\n",
        "        lbls = new_lab_list[ood_data.index(d)]\n",
        "        lbl_set.add(lbls)\n",
        "    if len(lbl_set)==1:\n",
        "        new_silver_data.extend(data)\n",
        "        lbls_to_add = [list(lbl_set)[0]]*len(data)\n",
        "        new_silver_class.extend(lbls_to_add)\n",
        "        remaining_data = list(set(clustered_sentences[cluster])-set(data))\n",
        "        cluster_wise_pts_remaining[cluster]=remaining_data\n",
        "        f =open(f'output_cluster_wise/cluster_{cluster}_rem_pts.txt','w')\n",
        "        for pts in remaining_data:\n",
        "            lbl = new_lab_list[ood_data.index(pts)]\n",
        "            f.write(pts.strip('\\n')+'\\t'+lbl)\n",
        "        f.close()\n",
        "    else:\n",
        "        not_good_clusters.append(cluster)\n"
      ],
      "metadata": {
        "id": "rVSaDUltbwOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_bad_clusters = bad_cluster+ not_good_clusters\n",
        "data_new_bad=[]\n",
        "random.seed(1)\n",
        "lbl_new_bad =[]\n",
        "new_added=0\n",
        "for cluster in set(new_bad_clusters):\n",
        "    data_bad_cluster = cluster_data_annotation[cluster]\n",
        "    lbl_bad_cluster = cluster_data_annotation_labels[cluster]\n",
        "    data = cluster_data_annotation[cluster]\n",
        "    #print(data_bad_cluster)\n",
        "    data_new_bad.extend(data_bad_cluster)\n",
        "    lbl_new_bad.extend(lbl_bad_cluster)\n",
        "    print(len(data_bad_cluster))\n",
        "    print(len(lbl_bad_cluster))\n",
        "    if len(data_bad_cluster)<5:\n",
        "        new_anns = random.sample(clustered_sentences[cluster],5-len(data_bad_cluster))\n",
        "        data.extend(new_anns)\n",
        "        new_added+=len(new_anns)\n",
        "        for anns in new_anns:\n",
        "            idx = ood_data.index(anns)\n",
        "            ann_label = new_lab_list[idx]\n",
        "            lbl_new_bad.append(ann_label)\n",
        "            data_new_bad.append(anns)\n",
        "    remaining_data = list(set(clustered_sentences[cluster])-set(data))\n",
        "    cluster_wise_pts_remaining[cluster]=remaining_data\n",
        "    f =open(f'output_cluster_wise/cluster_{cluster}_rem_pts.txt','w')\n",
        "    for pts in remaining_data:\n",
        "        lbl = new_lab_list[ood_data.index(pts)]\n",
        "        f.write(pts.strip('\\n')+'\\t'+lbl)\n",
        "    f.close()\n"
      ],
      "metadata": {
        "id": "gXYAoIi2by4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = new_silver_data+data_new_bad\n",
        "new_labels = new_silver_class+lbl_new_bad"
      ],
      "metadata": {
        "id": "p7YvJnsCb3BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_annotated_clusters=defaultdict(list)\n",
        "all_annotated_labels=defaultdict(list)\n",
        "for c in cluster_data_annotation:\n",
        "    annotated=set(clustered_sentences[c])-set(cluster_wise_pts_remaining[c])\n",
        "    total =list(annotated)\n",
        "    all_annotated_clusters[c]=total\n",
        "    for t in total:\n",
        "        lbl=new_lab_list[ood_data.index(t)]\n",
        "        all_annotated_labels[c].append(lbl)"
      ],
      "metadata": {
        "id": "Xy2paDtNb5Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_labelled_data=[]\n",
        "new_labelled_class=[]\n",
        "for data in all_annotated_clusters:\n",
        "    text = all_annotated_clusters[data]\n",
        "    lbl = all_annotated_labels[data]\n",
        "    new_labelled_data.extend(text)\n",
        "    new_labelled_class.extend(lbl)\n"
      ],
      "metadata": {
        "id": "JmJJQ5I_b6cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_annotated = all_annotations+count+new_added\n",
        "budget = ((new_classes)*10)-total_annotated"
      ],
      "metadata": {
        "id": "e74jMptGb76N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}